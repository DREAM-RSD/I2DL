{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知器与函数逼近\n",
    "* 假设我们把一个感知器网络中的所有的权重和偏置都乘以一个正的常数，$c>0$。证明网络的行为没有改变。\n",
    "* 对于一个感知器网络，给定其输入，并假设对于网络中任意感知器的输入 $x$，权重和偏置遵循$w\\cdot x+b\\ne 0$。现用S型神经元替换网络中所有的感知器，并将所欲的权重和偏置乘以一个正常数$c>0$。证明在$c\\to\\infty$的极限情况下，S型神经元网络的行为和感知器网络的完全一致。当一个感知器的$w\\cdot x+b= 0$时又为什么会不同？\n",
    "\n",
    "### 学习算法 梯度下降\n",
    "* 请证明为什么$ReLU$函数可以作为一种通用的神经网络激活函数？\n",
    "* 请展示并证明线性激活函数$s(z)=z$不能成为一个通用的激活函数？\n",
    "\n",
    "### 分类过程可视化\n",
    "* 假设我们将一个前馈神经网络中某一个神经元的神经元由$sigmoid$函数改为其他函数，这种情况下我们应该如何调整后向传播算法？\n",
    "* 假设我们将网络中所有的激活函数都设置为线性函数$s(z)=z$，这种情况下我们应该如何调整后向传播算法？\n",
    "\n",
    "### 学习过程可视化\n",
    "* 人们很容易对交叉熵函数的形式产生疑惑：它的形式究竟是$-[ylna+(1-y)ln(1-a)]$还是$-[alny+(1-a)ln(1-y)]$?在$y=0$或$y=1$时第二个表达式的结果会怎样？这个问题会困扰第一个表达式吗？为什么？\n",
    "* 在二元分类问题中我们总是假设真值$y$取值为$1$或$0$，而在回归问题中，$y$的值可以介于$0$和$1$之间。证明：交叉熵对所有训练数据在$\\sigma(z)=y$时仍然是最小化的，此时的交叉熵表示是：$$C=-\\frac 1n\\sum_{x}{}{[ylny+(1-y)ln(1-y)]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度学习\n",
    "* 证明以下内容：事实上，甚⾄有⼀种观点认为梯度下降法是求最⼩值的最优策略。假设我们正在努⼒去改变$\\Delta v$来让$C$尽可能地减小，这相当于最小化$\\Delta C \\approx \\nabla C \\cdot \\Delta v$。我们⾸先限制步⻓为⼩的固定值，即$\\begin{Vmatrix}\\Delta v\\end{Vmatrix}=\\epsilon,\\epsilon>0$。当步长固定时，我们要找到使得$C$减小最大的下降方向。可以证明，使得$\\nabla C \\cdot \\Delta v$取得最小值的$\\Delta v$为$\\Delta v=-\\eta\\nabla C$，这里$\\eta=\\epsilon /\\begin{Vmatrix}\\Delta v\\end{Vmatrix}$是由步长限制$\\begin{Vmatrix}\\Delta v\\end{Vmatrix}=\\epsilon$所决定的。因此，梯度下降法可以被视为一种在$C$下降最快的方向上做微小变化的方法。（提示：可以利⽤柯西-施⽡茨不等式）\n",
    "* 上一问题中已经解释了当$C$是⼆元及其多元函数的情况。那如果$C$是⼀个⼀元函数呢？你能给出梯度下降法在⼀元函数的⼏何解释么？\n",
    "\n",
    "### 过拟合与正则化\n",
    "考虑乘积$|w \\sigma'(wa+b)|$。假设有$|w\\sigma'(wa+b)|\\geq 1$。 \n",
    "* 证明这种情况只有在$|w|\\geq4$的时候才会出现。\n",
    "* 假设$|w|\\geq4$，考虑那些满足$|w \\sigma'(wa+b)| \\geq 1$的输入激活$a$集合。证明：满⾜上述条件的该集合能够充满⼀个不超过$\\frac{2}{|w|}\\ln\\left( \\frac{|w|(1+\\sqrt{1-4/|w|})}{2}-1\\right)$宽度的区间。\n",
    "* 数值上说明上述表达式在$|w|=6.9$时候去的最⾼值约 0.45。所以即使每个条件都满⾜，我们仍然有⼀个狭窄的输⼊激活区间，这样来避免消失的梯度问题。\n",
    "* 考虑⼀个单⼀输⼊的神经元$x$，对应的权重$w_1$，偏置$b$，输出上的权重$w_2$。证明，通过合理选择权重和偏置，我们可以确保$w_2\\sigma(w_1x+b)\\approx x$，其中$x \\in [0, 1]$。这样的神经元可⽤来作为⺓元使⽤，输出和输⼊相同（成⽐例） 。提⽰：可以重写$x = 1/2+\\Delta$，可以假设$w_1$很⼩，和在$w_1 \\Delta$使⽤泰勒级数展开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像分类-卷积网络\n",
    "* 梯度下降算法⼀个极端的版本是把⼩批量数据的⼤⼩设为1,此时该方法被称为在线、online、on-line、或者递增学习。在 online 学习中，神经⽹络在⼀个时刻只学习⼀个训练输⼊（正如⼈类做的） 。对⽐具有⼀个⼩批量输⼊大小为 20 的随机梯度下降，说出递增学习的⼀个优点和⼀个缺点。\n",
    "*  以分量形式写出⽅程$a' = \\sigma(w a + b)$，并验证它和计算 S 型神经元输出的结果$\\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}$相同。\n",
    "\n",
    "### 卷积网络可视化\n",
    "* 试着创建⼀个仅有两层的⽹络 —— ⼀个输⼊层和⼀个输出层，分别有 784 和 10 个神经元，没有隐藏层。⽤随机梯度下降算法训练⽹络。你能达到多少识别率？\n",
    "* 请证明在使用平方损失时，输出层的激活函数使用线性激活函数$s(z)=z$比使用sigmoid函数更容易缓解训练缓慢。\n",
    "\n",
    "### 图像对象检测\n",
    "* ⽬前， SGD ⽅法要求⽤⼾⼿动选择⽤于训练迭代期的数量。修改**“网络3”**以实现提前停⽌（early stopping）。\n",
    "* 请证明在输出层为何softmax的输出值与输出层所有的权重都相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像语义分割\n",
    "* 请通过公式推导证明交叉熵函数为什么能缓解学习缓慢的问题。\n",
    "* 当激活函数使用$ReLU$函数时，是否会出现梯度不稳定的问题？你能思考出一种改进网络的方法来缓解梯度不稳定的问题吗？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
